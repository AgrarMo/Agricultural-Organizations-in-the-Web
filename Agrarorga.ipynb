{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.6/3.8 MB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 9.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#%pip install bs4\n",
    "#%pip install chardet\n",
    "#%pip install networkx\n",
    "#%pip install Jinja2\n",
    "#%pip install pyvis\n",
    "#%pip install ipysigma\n",
    "#%pip install fa2\n",
    "#%pip install os\n",
    "#%pip install lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking robots.txt at: https://ticketcorner.ch/robots.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "################################################################################\n",
    "# Global crawler data\n",
    "################################################################################\n",
    "nodes = []  # each: { \"url\": str, \"id\": int, \"status\": str }\n",
    "links = []  # each: { \"source\": int, \"target\": int }\n",
    "\n",
    "visited_domains = set()       # which unified domains we have fully crawled\n",
    "visited_pages = set()         # full URLs visited\n",
    "edges_set = set()             # to avoid duplicate edges\n",
    "domain_crawl_count = 0\n",
    "max_domain_crawl_count = 100\n",
    "sleep_time = 3\n",
    "negative_list = [\"srf.ch\", \n",
    "                 \"nzz.ch\", \n",
    "                 \"tagesanzeiger.ch\",\n",
    "                 \"baselwandel.ch\",\n",
    "                 \"naturwissenschaften.ch\",\n",
    "                 \"20min.ch\",\n",
    "                 \"baz.ch\",\n",
    "                 \"bzbasel.ch\",\n",
    "                 \"klimageschichten.so.ch\",\n",
    "                 \"daten.stadt.sg.ch\",\n",
    "                 \"ag.ch\",\n",
    "                 \"mathias-binswanger.ch\",\n",
    "                 \"24heures.ch\",\n",
    "                 \"zentrumranft.ch\",\n",
    "                 \"redcross.ch\",\n",
    "                 \"letemps.ch\",\n",
    "                 \"migros-service.ch\",\n",
    "                 \"bernerzeitung.ch\",\n",
    "                 \"sergeandpeppers.ch\",\n",
    "                 \"silviodezanet.ch\",\n",
    "                 \"dergewerbeverein.ch\",\n",
    "                 \"berneroberlaender.ch\",\n",
    "                 \"frapp.ch\",\n",
    "                 \"ticketcorner.ch\"\n",
    "                ]\n",
    "\n",
    "positive_list = [\n",
    "    \"prospecierara.ch\"    \n",
    "    ]\n",
    "\n",
    "################################################################################\n",
    "# Update JSON file with nodes and edges\n",
    "################################################################################\n",
    "\n",
    "def load_existing_data(json_file=\"public/graph_data.json\"):\n",
    "    global nodes, links, visited_domains, visited_pages\n",
    "    if os.path.exists(json_file):\n",
    "        if os.path.getsize(json_file) > 0:  # Check if file is not empty\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # Convert \"label\" -> \"url\" if needed\n",
    "                raw_nodes = data.get(\"nodes\", [])\n",
    "                for n in raw_nodes:\n",
    "                    if \"url\" not in n and \"label\" in n:\n",
    "                        n[\"url\"] = n[\"label\"]\n",
    "                nodes[:] = raw_nodes\n",
    "                links[:] = data.get(\"edges\", [])\n",
    "\n",
    "                # Load visited sets if present\n",
    "                visited_domains_data = data.get(\"visited_domains\", [])\n",
    "                visited_pages_data = data.get(\"visited_pages\", [])\n",
    "                visited_domains = set(visited_domains_data) if visited_domains_data else set()\n",
    "                visited_pages = set(visited_pages_data) if visited_pages_data else set()\n",
    "        else:\n",
    "            print(f\"{json_file} is empty. Initializing data structures.\")\n",
    "            nodes = []\n",
    "            links = []\n",
    "            visited_domains = set()\n",
    "            visited_pages = set()\n",
    "    else:\n",
    "        print(f\"{json_file} does not exist. Initializing data structures.\")\n",
    "        nodes = []\n",
    "        links = []\n",
    "        visited_domains = set()\n",
    "        visited_pages = set()\n",
    "\n",
    "def generate_json_from_data(nodes, links, output_json=\"public/graph_data.json\"):\n",
    "    # Build nodes\n",
    "    nodes_list = []\n",
    "    for node in nodes:\n",
    "        nodes_list.append({\n",
    "            \"id\": int(node[\"id\"]),\n",
    "            \"label\": node[\"url\"],\n",
    "            \"status\": node[\"status\"],\n",
    "            \"size\": 3,\n",
    "            \"x\": node.get(\"x\", 0),\n",
    "            \"y\": node.get(\"y\", 0),\n",
    "        })\n",
    "\n",
    "    # Build edges\n",
    "    edges_list = []\n",
    "    for link in links:\n",
    "        edges_list.append({\n",
    "            \"source\": int(link[\"source\"]),\n",
    "            \"target\": int(link[\"target\"])\n",
    "        })\n",
    "\n",
    "    # Convert visited sets to lists for JSON\n",
    "    graph_data = {\n",
    "        \"nodes\": nodes_list,\n",
    "        \"edges\": edges_list,\n",
    "        \"visited_domains\": list(visited_domains),\n",
    "        \"visited_pages\": list(visited_pages)\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(graph_data, f, indent=4)\n",
    "\n",
    "    print(f\"Updated {output_json} with {len(nodes_list)} nodes and {len(edges_list)} edges, plus visited sets.\")\n",
    "\n",
    "def add_node_if_missing(domain_str):\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            return n[\"id\"]\n",
    "    new_id = len(nodes) + 1\n",
    "    nodes.append({\"url\": domain_str, \"id\": new_id, \"status\": \"Unknown\"})\n",
    "    return new_id\n",
    "\n",
    "def set_node_status(domain_str, status):\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            n[\"status\"] = status\n",
    "            return\n",
    "\n",
    "def get_node_id(domain_str):\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            return n[\"id\"]\n",
    "    raise KeyError(f\"Domain not found in nodes: {domain_str}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Utility functions\n",
    "################################################################################\n",
    "\n",
    "def unify_domain(url):\n",
    "    \"\"\"\n",
    "    Returns a domain string without scheme and without 'www.' prefix.\n",
    "    Example: 'https://www.urbanagriculturebasel.ch' -> 'urbanagriculturebasel.ch'\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    netloc = parsed.netloc.lower()\n",
    "    if netloc.startswith(\"www.\"):\n",
    "        netloc = netloc[4:]\n",
    "    return netloc\n",
    "\n",
    "def canonical_domain(url):\n",
    "    \"\"\"\n",
    "    For internal checks, returns the netloc in lowercase (still includes 'www.' if present).\n",
    "    Used to decide if a link is internal or external within BFS.\n",
    "    \"\"\"\n",
    "    return urlparse(url).netloc.lower()\n",
    "\n",
    "def normalize_url(url):\n",
    "    \"\"\"\n",
    "    Return a normalized full URL with scheme if missing, and lowercase netloc.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    scheme = parsed.scheme.lower() if parsed.scheme else \"https\"\n",
    "    netloc = parsed.netloc.lower()\n",
    "    return f\"{scheme}://{netloc}{parsed.path}\"\n",
    "\n",
    "def can_crawl(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        robots_url = urljoin(base_url, \"/robots.txt\")\n",
    "        print(f\"Checking robots.txt at: {robots_url}\")  # Debugging print\n",
    "\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        can_fetch = rp.can_fetch(\"*\", url)\n",
    "        print(f\"Can fetch {url}: {can_fetch}\")  # Debugging print\n",
    "        return can_fetch\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking robots.txt for {url}: {e}\")\n",
    "        return True\n",
    "\n",
    "def detect_encoding_and_decode(raw):\n",
    "    \"\"\"\n",
    "    Attempts to decode raw bytes, using chardet if available, otherwise utf-8 fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import chardet\n",
    "        result = chardet.detect(raw)\n",
    "        enc = result[\"encoding\"] or \"utf-8\"\n",
    "        return raw.decode(enc, errors=\"replace\")\n",
    "    except ImportError:\n",
    "        return raw.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def extract_links(url):\n",
    "    if not can_crawl(url):\n",
    "        print(f\"Robots.txt disallows crawling => {url}\")\n",
    "        return []\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=7)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.content\n",
    "\n",
    "        # First try html.parser, then fall back to lxml\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            print(f\"Parser error with html.parser. Falling back to 'lxml'. Error: {e}\")\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "        valid_links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            raw_href = a[\"href\"]\n",
    "            try:\n",
    "                merged_url = urljoin(url, raw_href)\n",
    "                valid_links.append(merged_url)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return valid_links\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to extract links from {url}: {e}\")\n",
    "        return []\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Unicode decode error for {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_visible_text(html):\n",
    "    \"\"\"\n",
    "    Remove non-visible tags like <script>, <style>, <meta>, etc.\n",
    "    Then get only the text from <body>.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Remove tags not considered main content\n",
    "    for tag_name in [\"script\", \"style\", \"meta\", \"head\", \"noscript\", \"link\"]:\n",
    "        for t in soup.find_all(tag_name):\n",
    "            t.decompose()\n",
    "\n",
    "    # If you need to remove display:none elements:\n",
    "    # for hidden in soup.select(\"[style*='display:none']\"):\n",
    "    #     hidden.decompose()\n",
    "\n",
    "    # Some pages may not have a <body> tag; handle that gracefully\n",
    "    body = soup.body\n",
    "    if body is not None:\n",
    "        text = body.get_text(separator=\" \", strip=True)\n",
    "    else:\n",
    "        # fallback: entire soup\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def contains_keyword(domain_url, keyword_list):\n",
    "    \"\"\"\n",
    "    Check if the domain_url page text contains at least one of the given keywords \n",
    "    as a full standalone word. Use regex with negative lookbehind/lookahead \n",
    "    and print out what was matched for debugging.\n",
    "    \"\"\"\n",
    "    if not can_crawl(domain_url):\n",
    "        return False\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # Use the same headers for your keyword check\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(domain_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=7)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Remove tags containing non-user-facing text\n",
    "        for tag_name in [\"script\", \"style\", \"head\", \"title\", \"meta\", \"noscript\"]:\n",
    "            for tag in soup.find_all(tag_name):\n",
    "                tag.decompose()\n",
    "\n",
    "        # Join all remaining text\n",
    "        visible_text = ' '.join(soup.stripped_strings).lower()\n",
    "\n",
    "        # Check if any keyword is present\n",
    "        return any(kw.lower() in visible_text for kw in keyword_list)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"contains_keyword failed for {domain_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_node_if_missing(domain_str):\n",
    "    \"\"\"\n",
    "    Use the 'domain_str' from unify_domain() as unique node key.\n",
    "    If it doesn't exist in 'nodes', add it. Return node ID.\n",
    "    \"\"\"\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            return n[\"id\"]\n",
    "    new_id = len(nodes) + 1\n",
    "    nodes.append({\"url\": domain_str, \"id\": new_id, \"status\": \"Unknown\"})\n",
    "    return new_id\n",
    "\n",
    "def set_node_status(domain_str, status):\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            n[\"status\"] = status\n",
    "            return\n",
    "\n",
    "def get_node_id(domain_str):\n",
    "    for n in nodes:\n",
    "        if n[\"url\"] == domain_str:\n",
    "            return n[\"id\"]\n",
    "    raise KeyError(f\"Domain not found in nodes: {domain_str}\")\n",
    "\n",
    "################################################################################\n",
    "# BFS domain crawl\n",
    "################################################################################\n",
    "def bfs_crawl_domain(domain_str, depth=1):\n",
    "    print(f\"\\n--- Crawling domain: {domain_str} (Domain #{domain_crawl_count}) ---\")\n",
    "    if domain_crawl_count > max_domain_crawl_count:\n",
    "        print(\"Limit of crawlable domains reached. Stopping.\")\n",
    "        return\n",
    "\n",
    "    start_page = f\"https://{domain_str}\"\n",
    "    visited_pages.add(start_page)\n",
    "    queue = deque([(start_page, 0)])\n",
    "    \n",
    "    # We assume the node for this domain already exists (e.g., after add_node_if_missing).\n",
    "    try:\n",
    "        source_id = get_node_id(domain_str)\n",
    "    except KeyError:\n",
    "        source_id = add_node_if_missing(domain_str)\n",
    "\n",
    "    while queue:\n",
    "        page_url, lvl = queue.popleft()\n",
    "        if lvl > depth:\n",
    "            break\n",
    "\n",
    "        lower = page_url.lower()\n",
    "        if lower.endswith(\".pdf\"):\n",
    "            print(f\"Skipping PDF: {page_url}\")\n",
    "            continue\n",
    "        if lower.endswith(\".jpg\"):\n",
    "            print(f\"Skipping JPG: {page_url}\")\n",
    "            continue\n",
    "        if lower.endswith(\".mp4\"):\n",
    "            print(f\"Skipping MP4: {page_url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nCrawling page: {page_url} (depth={lvl})\")\n",
    "        links_found = extract_links(page_url)\n",
    "        print(f\"Found {len(links_found)} links at {page_url}\")\n",
    "\n",
    "        # If it's the homepage (lvl=0) and we see > 200 links, classify + skip further\n",
    "        if lvl == 0 and len(links_found) > 200:\n",
    "            print(\"Homepage has more than 200 links, treating as 'Webshop-like' and skipping BFS.\")\n",
    "            set_node_status(domain_str, \"Relevant, but possibly Webshop-like with too many links\")\n",
    "            break\n",
    "\n",
    "        for link in links_found:\n",
    "            link_unified = unify_domain(link).strip()\n",
    "            if not link_unified:\n",
    "                # Domain is empty or invalid\n",
    "                continue\n",
    "\n",
    "            if link_unified != domain_str:\n",
    "                # External link => only add node and link if you actually want it\n",
    "                try:\n",
    "                    target_id = get_node_id(link_unified)\n",
    "                except KeyError:\n",
    "                    target_id = add_node_if_missing(link_unified)\n",
    "                if (source_id, target_id) not in edges_set:\n",
    "                    edges_set.add((source_id, target_id))\n",
    "                    links.append({\"source\": source_id, \"target\": target_id})\n",
    "\n",
    "            else:\n",
    "                # Internal link => BFS deeper\n",
    "                norm = normalize_url(link)\n",
    "                if norm not in visited_pages and lvl < depth:\n",
    "                    visited_pages.add(norm)\n",
    "                    queue.append((norm, lvl + 1))\n",
    "\n",
    "    # Finally, update the JSON output after finishing this domain\n",
    "    generate_json_from_data(nodes, links, \"public/graph_data.json\")\n",
    "\n",
    "################################################################################\n",
    "# Main\n",
    "################################################################################\n",
    "def main():\n",
    "    keyword_list = [\"landwirtschaft\", \"landwirtschaftlich\",\"agriculture\",\"agricoltura\",\"farming\",\"agrar\",\"fattoria\",\"agricole\",\"ferme\",\"paysan\",\"plouc\",\"bauer\"]\n",
    "\n",
    "    # Load existing data from JSON\n",
    "    load_existing_data()\n",
    "\n",
    "    start_url = \"https://ticketcorner.ch/\"\n",
    "    start_unified = unify_domain(start_url)\n",
    "    add_node_if_missing(start_unified)\n",
    "\n",
    "    # If domain is in positive_list, skip keyword check => mark Relevant + BFS\n",
    "    if any(pdom in start_unified for pdom in positive_list):\n",
    "        set_node_status(start_unified, \"Relevant\")\n",
    "        bfs_crawl_domain(start_unified, depth=1)\n",
    "    elif any(nd in start_unified for nd in negative_list):\n",
    "        set_node_status(start_unified, \"Negativliste\")\n",
    "    elif start_unified.endswith(\".ch\"):\n",
    "        keyword_check = contains_keyword(start_url, keyword_list)\n",
    "        if keyword_check is None:\n",
    "            set_node_status(start_unified, \"could not test keyword\")\n",
    "        elif keyword_check:\n",
    "            set_node_status(start_unified, \"Relevant\")\n",
    "            bfs_crawl_domain(start_unified, depth=1)\n",
    "        else:\n",
    "            set_node_status(start_unified, \"Kein Bezug zu Landwirtschaft\")\n",
    "    else:\n",
    "        set_node_status(start_unified, \"Nicht in der Schweiz\")\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(nodes) and domain_crawl_count < max_domain_crawl_count:\n",
    "        node = nodes[idx]\n",
    "        idx += 1\n",
    "\n",
    "        if node[\"url\"] in visited_domains:\n",
    "            continue\n",
    "        \n",
    "        if node[\"status\"] in (\"Start\", \"Relevant\"):\n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "            continue\n",
    "\n",
    "        dom_str = node[\"url\"]\n",
    "\n",
    "        if dom_str in visited_domains:\n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "            continue\n",
    "\n",
    "        if any(nd in dom_str for nd in negative_list):\n",
    "            set_node_status(dom_str, \"Negativliste\")\n",
    "            visited_domains.add(dom_str)  # Mark visited\n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "            continue\n",
    "\n",
    "        if any(pdom in dom_str for pdom in positive_list):\n",
    "            set_node_status(dom_str, \"Relevant\") \n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "            bfs_crawl_domain(dom_str, depth=1)\n",
    "            continue\n",
    "\n",
    "        if dom_str.endswith(\".ch\"):\n",
    "            keyword_check = contains_keyword(f\"https://{dom_str}\", keyword_list)\n",
    "            if keyword_check is None:\n",
    "                set_node_status(dom_str, \"could not test keyword\")\n",
    "            elif keyword_check:\n",
    "                set_node_status(dom_str, \"Relevant\")\n",
    "                bfs_crawl_domain(dom_str, depth=1)\n",
    "            else:\n",
    "                set_node_status(dom_str, \"Kein Bezug zu Landwirtschaft\")\n",
    "            visited_domains.add(dom_str)  # Mark visited here as well\n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "        else:\n",
    "            set_node_status(dom_str, \"Nicht in der Schweiz\")\n",
    "            visited_domains.add(dom_str)  # Mark visited\n",
    "            generate_json_from_data(nodes, links, 'public/graph_data.json')  # Update JSON\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Graph_data-filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered nodes: 1925\n",
      "Filtered node IDs: {1, 9, 12, 8204, 20, 22, 24, 29, 8224, 8229, 8235, 53, 61, 79, 90, 94, 97, 100, 8292, 8293, 111, 8303, 125, 134, 167, 8363, 8367, 193, 8388, 8396, 214, 215, 8408, 218, 219, 8410, 223, 8416, 226, 234, 236, 8430, 239, 8431, 243, 244, 8438, 250, 253, 264, 271, 273, 275, 277, 281, 284, 8476, 8477, 287, 290, 295, 296, 300, 301, 8492, 303, 304, 306, 307, 323, 324, 330, 8522, 8523, 8524, 339, 342, 8551, 364, 8559, 8572, 8573, 8586, 8589, 8590, 8600, 8602, 415, 418, 431, 432, 8623, 8632, 8633, 8634, 452, 453, 8646, 8647, 460, 462, 8655, 464, 8657, 8659, 8660, 8661, 8662, 471, 8665, 474, 8666, 8668, 478, 484, 8676, 8693, 506, 8700, 8701, 8702, 519, 520, 8712, 8713, 533, 535, 536, 8744, 8763, 577, 586, 8779, 8781, 591, 592, 594, 596, 597, 8790, 599, 8791, 601, 8805, 616, 8809, 8814, 8815, 625, 8819, 635, 636, 8839, 649, 8845, 667, 8866, 8868, 684, 685, 8878, 8880, 701, 702, 703, 706, 8898, 708, 709, 714, 8908, 718, 719, 8910, 8913, 8915, 8916, 8919, 728, 8921, 8925, 735, 736, 737, 739, 746, 748, 749, 8942, 752, 758, 760, 762, 8954, 764, 766, 767, 768, 769, 8958, 8962, 773, 774, 775, 776, 778, 779, 782, 784, 785, 8977, 788, 789, 790, 8987, 8989, 816, 817, 9008, 843, 845, 9037, 848, 9044, 853, 855, 856, 857, 9048, 859, 863, 866, 867, 868, 869, 873, 9067, 887, 889, 890, 895, 896, 897, 9087, 902, 903, 904, 906, 907, 908, 909, 910, 911, 912, 9099, 9104, 915, 916, 917, 920, 921, 925, 928, 933, 935, 942, 9137, 949, 9143, 9144, 9146, 955, 957, 959, 9156, 9160, 971, 973, 9166, 976, 9176, 988, 9185, 9186, 998, 999, 9191, 1003, 1004, 1005, 9195, 9197, 1008, 1009, 9198, 9199, 9200, 9201, 9202, 9203, 1016, 9205, 9206, 9207, 1020, 9208, 9209, 9212, 9213, 9214, 1026, 1027, 9219, 1030, 1032, 1033, 9232, 9234, 9235, 9237, 9238, 9240, 9243, 9246, 9247, 1057, 1058, 9250, 1062, 9259, 1071, 9265, 9266, 9268, 9270, 1082, 9274, 9277, 9278, 1089, 1098, 1103, 1105, 1106, 9297, 1112, 1114, 9308, 1117, 1118, 1120, 1123, 1126, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1139, 9331, 1141, 9332, 1144, 1145, 1146, 1147, 1151, 1152, 1153, 1155, 1159, 1160, 1163, 1164, 1166, 1168, 1169, 1173, 1174, 1177, 1178, 9369, 1181, 1187, 1191, 1192, 1195, 1196, 1197, 1198, 9390, 1201, 1202, 9393, 1204, 1212, 1213, 1215, 1217, 1219, 9417, 9418, 9424, 1234, 1237, 1240, 1242, 9437, 9440, 9445, 1255, 1257, 1258, 9452, 9453, 9455, 1265, 1266, 9458, 1268, 9461, 1271, 9464, 1273, 9465, 9466, 9467, 9468, 1278, 9469, 9473, 9474, 1283, 1284, 9480, 1295, 9493, 1303, 1304, 1305, 9513, 9518, 1348, 9549, 1367, 1368, 9563, 1372, 1373, 9564, 9566, 1376, 9567, 1380, 9575, 1390, 9582, 9583, 9585, 9586, 1418, 9613, 9614, 1427, 1428, 9622, 9623, 9633, 1445, 9655, 9662, 9665, 9666, 9669, 9670, 9671, 9673, 9675, 9678, 1488, 1492, 9684, 1494, 1495, 9686, 9689, 9697, 9700, 9705, 1520, 9712, 9721, 1531, 9726, 1538, 1539, 1542, 1543, 1550, 1554, 1561, 1562, 1564, 1567, 1571, 9763, 1575, 9768, 1580, 1583, 1584, 1585, 9775, 9778, 1588, 1589, 1591, 9783, 1593, 1600, 9793, 1602, 1606, 1607, 1609, 1614, 1616, 1617, 9812, 1627, 1632, 9825, 1638, 1639, 1640, 9833, 9834, 1645, 1649, 1653, 1655, 9847, 1657, 1658, 9851, 1660, 1661, 1662, 1663, 9854, 1666, 9862, 1671, 1672, 9864, 9866, 1676, 1677, 9871, 1680, 1681, 1682, 1683, 1684, 9874, 1687, 9880, 9881, 9882, 1691, 1693, 1695, 9887, 1697, 1698, 1705, 9898, 9903, 1713, 1714, 1715, 9907, 1720, 1726, 1727, 1728, 1731, 1733, 1735, 1738, 1739, 1744, 1753, 1756, 1761, 1762, 1764, 1765, 1766, 1769, 1770, 1772, 1774, 1775, 9966, 9967, 9971, 9972, 1781, 9973, 9978, 9979, 1788, 9980, 1791, 1793, 1795, 9987, 9989, 1801, 1802, 1804, 1809, 10004, 10005, 1814, 10019, 10022, 10025, 1836, 1838, 1840, 1845, 1853, 1854, 10055, 1867, 10060, 1870, 1875, 1878, 10070, 1888, 10084, 1895, 1901, 1902, 10097, 1906, 10098, 1916, 1921, 1923, 1928, 1932, 1934, 10128, 1937, 1938, 1939, 1941, 1944, 1946, 1947, 1954, 10149, 1958, 1966, 1972, 10165, 1975, 10171, 10174, 1984, 10179, 1988, 10180, 10181, 10183, 10185, 10192, 2013, 2014, 10207, 10213, 10219, 10221, 10224, 10228, 10233, 2042, 10235, 2055, 10248, 10259, 10261, 10267, 10268, 10271, 10280, 10281, 10285, 10288, 10291, 10292, 2107, 10299, 10300, 2115, 2116, 10313, 2122, 2130, 2131, 10326, 2142, 10344, 2158, 10351, 10355, 10357, 2166, 2169, 2171, 2172, 2173, 2174, 2176, 10368, 2178, 10369, 10374, 10378, 2187, 2195, 10393, 2207, 10400, 10401, 10409, 10412, 2221, 10413, 10426, 2235, 10430, 10437, 2250, 2264, 2270, 10463, 2298, 2300, 10494, 10495, 2310, 10506, 2321, 2364, 2378, 2379, 10571, 10575, 10580, 10581, 2400, 2406, 2412, 10611, 2420, 10625, 10627, 10628, 10629, 10632, 10633, 2443, 2447, 2450, 2452, 2454, 10647, 10648, 10651, 2464, 10658, 2468, 10662, 10664, 10667, 10669, 10673, 10677, 10678, 10679, 2489, 10681, 10682, 10692, 10693, 10697, 10698, 10699, 10703, 10708, 2518, 10713, 10714, 10719, 10723, 2551, 2560, 10755, 2569, 10763, 2572, 2580, 2592, 2593, 2594, 10787, 2598, 2599, 2604, 2607, 2609, 2623, 2634, 2636, 2653, 2656, 2658, 10852, 2662, 10858, 10859, 10860, 10861, 2673, 2675, 2677, 10870, 2680, 10875, 10876, 10880, 2689, 10882, 2691, 2702, 2705, 2711, 2714, 10908, 2720, 10915, 10917, 10918, 2728, 2729, 2731, 10926, 10927, 10930, 10933, 2747, 10940, 10944, 10949, 10950, 10951, 10954, 2763, 10960, 10961, 10962, 10963, 2777, 2779, 2780, 2781, 2782, 10971, 10975, 2785, 2786, 10976, 2788, 10979, 2791, 2792, 10983, 10986, 10987, 10990, 2803, 2804, 2805, 2807, 2814, 11008, 2828, 2832, 2834, 2838, 2839, 2840, 2848, 11040, 2853, 11046, 2855, 11049, 2860, 2861, 11055, 2865, 2866, 2868, 11060, 2872, 11064, 2875, 11069, 2878, 2880, 2882, 2883, 2884, 2885, 2886, 2888, 11081, 2898, 11090, 2901, 11096, 2908, 2912, 2913, 11105, 11117, 11118, 11119, 2928, 11120, 2931, 11125, 11131, 2940, 2941, 2942, 2943, 11133, 2945, 11134, 11136, 2948, 11138, 11145, 11150, 11152, 11153, 11158, 11159, 11160, 11161, 2971, 2972, 11163, 11164, 11166, 11176, 11180, 11181, 11182, 11184, 11188, 11192, 11196, 11197, 11198, 11199, 11208, 11217, 11224, 11226, 11228, 11229, 11234, 3047, 3048, 3054, 3055, 11254, 3065, 11260, 3072, 3074, 11268, 11277, 3094, 3095, 3098, 3101, 11293, 11296, 3105, 11298, 3113, 3114, 11307, 11311, 3120, 11313, 11314, 11316, 3126, 11319, 11321, 11322, 11327, 11334, 11335, 11338, 11343, 3155, 11354, 11355, 11356, 11358, 3168, 11376, 3187, 11379, 3189, 11381, 3196, 3198, 11392, 11393, 11394, 11396, 3210, 11402, 11403, 11404, 3216, 3217, 3219, 3224, 3226, 3229, 3230, 11422, 3232, 3233, 3238, 3245, 3252, 3254, 3259, 3262, 11505, 3330, 3337, 3391, 3402, 3403, 3404, 3405, 3406, 3407, 3409, 11602, 11605, 11612, 3424, 3427, 3428, 3429, 3432, 3434, 11628, 3439, 3442, 3443, 3444, 3446, 11643, 3453, 3454, 3455, 11647, 3458, 3459, 11654, 11656, 11657, 11658, 3467, 3469, 3470, 3471, 11663, 3473, 3474, 11664, 11667, 3479, 11671, 11680, 11681, 3492, 11684, 11686, 11687, 11688, 3498, 3501, 3502, 11695, 3504, 11696, 3511, 11708, 11714, 11715, 11716, 3527, 3530, 3533, 11725, 3538, 11730, 11732, 11737, 3549, 11745, 11746, 11747, 11749, 11750, 11756, 3566, 11761, 3575, 11783, 3595, 3599, 3605, 11797, 11806, 3616, 11815, 11817, 3635, 3636, 11830, 3639, 3640, 11853, 3662, 11854, 11863, 3681, 11885, 11890, 11892, 11893, 3749, 3757, 11955, 3770, 3772, 11966, 11972, 11973, 11975, 11976, 11977, 3787, 11986, 3805, 3806, 12000, 12009, 12012, 12013, 12014, 12015, 12024, 12026, 12031, 12035, 12036, 12041, 12043, 12044, 12057, 3868, 12065, 3877, 12073, 3883, 12075, 3886, 3887, 3888, 3889, 12080, 12082, 3896, 12089, 12092, 12093, 3902, 3905, 3906, 3907, 12100, 3909, 12102, 3913, 3914, 3918, 12112, 3922, 3923, 3927, 3928, 3931, 12126, 3936, 3938, 3939, 3940, 3941, 3942, 3943, 3947, 3948, 3950, 12142, 12143, 3955, 3956, 12149, 12151, 3961, 12155, 3966, 3967, 12160, 3969, 3970, 3971, 3972, 12173, 12178, 3989, 3990, 12182, 3992, 3993, 3994, 12187, 12191, 4001, 12195, 4007, 4009, 12202, 4015, 4016, 12209, 4018, 4019, 4020, 4021, 4023, 4030, 12227, 12229, 4054, 12263, 4080, 12273, 12284, 12297, 4107, 4109, 4112, 4113, 4114, 4115, 4116, 4117, 12310, 4120, 4121, 4122, 4128, 4137, 4139, 4142, 4152, 4153, 4155, 4158, 4160, 12352, 4163, 4164, 4167, 4174, 4176, 12370, 12371, 12376, 12378, 12379, 12380, 12384, 12385, 12386, 4196, 12389, 4198, 12395, 12398, 4207, 4210, 4211, 12406, 4218, 12411, 4226, 4227, 4228, 4230, 12424, 12427, 12428, 12429, 12434, 4244, 12438, 4248, 12443, 12446, 12447, 4258, 4281, 12499, 12506, 12511, 12515, 12518, 12519, 12521, 12522, 12524, 12526, 12527, 12530, 4362, 4363, 12555, 4368, 4373, 4375, 4384, 4391, 12583, 4394, 4398, 4402, 12596, 4407, 4408, 12599, 4410, 12604, 4417, 4419, 4422, 4424, 4428, 4430, 12623, 4432, 12624, 12625, 4436, 12633, 12634, 4444, 12637, 12648, 12664, 12676, 4487, 12690, 12697, 12712, 4522, 4523, 4524, 12714, 12724, 4533, 12725, 4555, 12749, 12759, 12763, 12764, 12766, 12771, 4581, 4585, 12786, 4596, 12792, 4604, 4611, 4621, 4623, 12815, 12816, 4627, 12819, 12820, 12821, 4631, 4633, 12825, 12830, 12831, 12833, 4647, 12842, 12844, 12846, 12850, 12851, 12852, 4662, 12854, 4664, 12855, 12857, 12859, 12860, 12862, 12863, 12864, 12865, 12869, 12871, 12872, 12873, 12874, 4683, 12875, 12877, 12880, 12883, 4694, 12888, 12889, 12890, 4702, 12895, 4708, 4710, 4722, 4732, 4739, 12938, 12941, 4750, 4753, 4756, 12952, 4762, 4763, 12954, 4767, 4770, 4772, 4776, 4779, 4780, 4784, 4789, 4790, 4798, 4822, 4824, 4827, 4830, 4831, 4832, 4838, 4843, 4846, 4859, 4862, 4864, 4885, 4886, 4887, 4888, 4889, 4890, 4892, 4900, 4908, 4910, 4913, 4917, 4927, 4929, 4931, 4938, 4940, 4946, 4947, 4948, 4953, 4956, 4966, 4970, 4973, 4980, 4990, 4992, 4997, 4999, 5002, 5003, 5004, 5010, 5012, 5015, 5016, 5019, 5020, 5021, 5024, 5028, 5083, 5086, 5089, 5090, 5091, 5122, 5125, 5137, 5138, 5166, 5184, 5185, 5187, 5195, 5215, 5217, 5218, 5223, 5228, 5240, 5241, 5243, 5250, 5251, 5259, 5260, 5261, 5270, 5275, 5283, 5287, 5291, 5296, 5298, 5306, 5316, 5344, 5345, 5347, 5358, 5363, 5364, 5365, 5366, 5368, 5369, 5370, 5381, 5383, 5387, 5388, 5389, 5390, 5393, 5394, 5396, 5397, 5399, 5401, 5402, 5405, 5406, 5407, 5408, 5410, 5419, 5423, 5434, 5439, 5448, 5450, 5456, 5457, 5464, 5465, 5467, 5468, 5472, 5476, 5477, 5479, 5482, 5485, 5493, 5498, 5501, 5503, 5504, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5521, 5525, 5527, 5548, 5551, 5553, 5554, 5555, 5556, 5558, 5561, 5567, 5589, 5592, 5602, 5604, 5609, 5610, 5612, 5614, 5615, 5618, 5622, 5623, 5624, 5630, 5639, 5662, 5663, 5669, 5687, 5689, 5691, 5696, 5697, 5700, 5713, 5715, 5723, 5732, 5736, 5740, 5764, 5777, 5807, 5820, 5846, 5858, 5865, 5872, 5891, 5897, 5916, 5950, 5977, 5995, 5999, 6003, 6009, 6025, 6030, 6032, 6042, 6043, 6047, 6068, 6085, 6111, 6147, 6194, 6200, 6202, 6205, 6218, 6226, 6246, 6251, 6256, 6259, 6266, 6273, 6277, 6280, 6285, 6292, 6314, 6319, 6320, 6328, 6334, 6335, 6342, 6356, 6358, 6362, 6364, 6367, 6369, 6399, 6406, 6415, 6416, 6418, 6420, 6426, 6427, 6428, 6435, 6437, 6447, 6449, 6454, 6455, 6456, 6463, 6465, 6471, 6508, 6516, 6532, 6556, 6567, 6595, 6621, 6625, 6630, 6637, 6638, 6650, 6660, 6665, 6670, 6671, 6676, 6685, 6706, 6749, 6762, 6768, 6773, 6793, 6796, 6798, 6801, 6810, 6818, 6822, 6867, 6879, 6891, 6893, 6899, 6921, 6932, 6933, 6942, 6944, 6966, 6971, 6983, 7006, 7019, 7024, 7026, 7033, 7036, 7042, 7046, 7052, 7053, 7057, 7059, 7060, 7071, 7083, 7085, 7088, 7094, 7095, 7097, 7098, 7107, 7108, 7124, 7134, 7135, 7140, 7144, 7159, 7167, 7177, 7187, 7193, 7197, 7225, 7226, 7242, 7255, 7274, 7286, 7365, 7376, 7396, 7429, 7437, 7440, 7457, 7470, 7472, 7475, 7493, 7505, 7509, 7542, 7543, 7545, 7546, 7547, 7552, 7564, 7574, 7578, 7580, 7581, 7591, 7594, 7597, 7620, 7622, 7646, 7674, 7677, 7679, 7687, 7689, 7701, 7704, 7705, 7706, 7707, 7711, 7726, 7744, 7758, 7760, 7765, 7768, 7770, 7774, 7784, 7786, 7789, 7792, 7800, 7803, 7809, 7814, 7815, 7823, 7865, 7888, 7894, 7937, 7988, 7990, 8014, 8022, 8025, 8048, 8050, 8069, 8166, 8181}\n",
      "Number of filtered edges: 8042\n",
      "Filtered graph data has been written to graph_data_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the original graph data\n",
    "with open('public/graph_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter nodes with \"status\" containing \"Relevant\"\n",
    "filtered_nodes = [node for node in data['nodes'] if 'Relevant' in node['status']]\n",
    "\n",
    "# Debugging: Print the number of filtered nodes\n",
    "print(f\"Number of filtered nodes: {len(filtered_nodes)}\")\n",
    "\n",
    "# Get the IDs of the filtered nodes\n",
    "filtered_node_ids = {node['id'] for node in filtered_nodes}\n",
    "\n",
    "# Debugging: Print the filtered node IDs\n",
    "print(f\"Filtered node IDs: {filtered_node_ids}\")\n",
    "\n",
    "# Filter edges that connect the filtered nodes\n",
    "filtered_edges = [edge for edge in data['edges'] if edge['source'] in filtered_node_ids and edge['target'] in filtered_node_ids]\n",
    "\n",
    "# Debugging: Print the number of filtered edges\n",
    "print(f\"Number of filtered edges: {len(filtered_edges)}\")\n",
    "\n",
    "# Create the filtered graph data\n",
    "filtered_data = {\n",
    "    'nodes': filtered_nodes,\n",
    "    'edges': filtered_edges\n",
    "}\n",
    "\n",
    "# Write the filtered data to a new JSON file\n",
    "with open('public/graph_data_filtered.json', 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(\"Filtered graph data has been written to graph_data_filtered.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
